{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1lo3eNnPTQZqibM9Zj6pM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/galencky/whisper-stt-project/blob/main/Whisper_STT_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2) Create (if it doesn't exist) a subfolder named \"Whisper-STT-project\"\n",
        "import os\n",
        "\n",
        "PROJECT_SUBFOLDER = \"/content/drive/MyDrive/Whisper-STT-project\"\n",
        "os.makedirs(PROJECT_SUBFOLDER, exist_ok=True)\n",
        "print(f\"‚úÖ Project folder ready at: {PROJECT_SUBFOLDER}\")\n",
        "\n",
        "# 3) Install dependencies via pip (use the PyPI release of google-generativeai)\n",
        "!pip install --upgrade openai-whisper tqdm google-generativeai requests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nZRjrUzQGo7",
        "outputId": "da999c41-109b-48e9-9aaa-11de7d07be67"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Project folder ready at: /content/drive/MyDrive/Whisper-STT-project\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20240930)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.169.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Adjust this if your Drive folder is named differently ‚îÄ‚îÄ‚îÄ\n",
        "PROJECT_SUBFOLDER = \"/content/drive/MyDrive/Whisper-STT-project\"\n",
        "\n",
        "BASE_DIR        = Path(PROJECT_SUBFOLDER)\n",
        "INBOX_DIR       = BASE_DIR / \"inbox\"\n",
        "PROCESSED_DIR   = BASE_DIR / \"processed\"\n",
        "TRANSCRIPTS_DIR = BASE_DIR / \"transcripts\"\n",
        "#MODEL_CACHE_DIR = BASE_DIR / \"models\"\n",
        "PARSED_DIR      = BASE_DIR / \"parsed\"\n",
        "MARKDOWN_DIR    = BASE_DIR / \"markdown\"\n",
        "UPLOADED_DIR    = BASE_DIR / \"uploaded\"\n",
        "\n",
        "# Create all folders if they don't exist\n",
        "for folder in (\n",
        "    BASE_DIR,\n",
        "    INBOX_DIR,\n",
        "    PROCESSED_DIR,\n",
        "    TRANSCRIPTS_DIR,\n",
        "#    MODEL_CACHE_DIR,\n",
        "    PARSED_DIR,\n",
        "    MARKDOWN_DIR,\n",
        "    UPLOADED_DIR,\n",
        "):\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Folder structure under Drive is ready:\")\n",
        "print(f\"  BASE_DIR       = {BASE_DIR}\")\n",
        "print(f\"  INBOX_DIR      = {INBOX_DIR}\")\n",
        "print(f\"  PROCESSED_DIR  = {PROCESSED_DIR}\")\n",
        "print(f\"  TRANSCRIPTS_DIR= {TRANSCRIPTS_DIR}\")\n",
        "#print(f\"  MODEL_CACHE_DIR= {MODEL_CACHE_DIR}\")\n",
        "print(f\"  PARSED_DIR     = {PARSED_DIR}\")\n",
        "print(f\"  MARKDOWN_DIR   = {MARKDOWN_DIR}\")\n",
        "print(f\"  UPLOADED_DIR   = {UPLOADED_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2fT0COeQKD2",
        "outputId": "b11f1e9d-3873-4bb0-e802-fed6af517947"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Folder structure under Drive is ready:\n",
            "  BASE_DIR       = /content/drive/MyDrive/Whisper-STT-project\n",
            "  INBOX_DIR      = /content/drive/MyDrive/Whisper-STT-project/inbox\n",
            "  PROCESSED_DIR  = /content/drive/MyDrive/Whisper-STT-project/processed\n",
            "  TRANSCRIPTS_DIR= /content/drive/MyDrive/Whisper-STT-project/transcripts\n",
            "  PARSED_DIR     = /content/drive/MyDrive/Whisper-STT-project/parsed\n",
            "  MARKDOWN_DIR   = /content/drive/MyDrive/Whisper-STT-project/markdown\n",
            "  UPLOADED_DIR   = /content/drive/MyDrive/Whisper-STT-project/uploaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime, shutil, sys, time\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import whisper\n",
        "\n",
        "AUDIO_EXT = {'.wav', '.mp3', '.m4a', '.flac', '.ogg', '.webm'}\n",
        "\n",
        "def _now():\n",
        "    \"\"\"Log-friendly timestamp.\"\"\"\n",
        "    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "def _fmt_ts(seconds: float) -> str:\n",
        "    \"\"\"float seconds ‚Üí HH:MM:SS.mmm string.\"\"\"\n",
        "    h, m = divmod(int(seconds), 3600)\n",
        "    m, s = divmod(m, 60)\n",
        "    ms = int((seconds - int(seconds)) * 1000)\n",
        "    return f\"{h:02d}:{m:02d}:{s:02d}.{ms:03d}\"\n",
        "\n",
        "def save_transcript(result: dict, out_path: Path):\n",
        "    \"\"\"\n",
        "    Write Whisper's segment list to a .txt file.\n",
        "    Each line: [start ‚Üí end] text\n",
        "    \"\"\"\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for seg in result[\"segments\"]:\n",
        "            f.write(f\"[{_fmt_ts(seg['start'])} ‚Üí {_fmt_ts(seg['end'])}] \"\n",
        "                    f\"{seg['text'].strip()}\\n\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Load Whisper Model (no persistent cache) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "start = time.time()\n",
        "print(f\"[{_now()}] Loading Whisper large-v3 ‚Ä¶\")\n",
        "model = whisper.load_model(\"large-v3\")  # no download_root ‚Üí uses default HF cache in /root/.cache/\n",
        "print(f\"[{_now()}] Model ready (took {time.time() - start:.1f} s)\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4NEFO50QLTN",
        "outputId": "e3f3f964-8038-4eed-85a1-216ff99c11d6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-05-31 03:12:05] Loading Whisper large-v3 ‚Ä¶\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.88G/2.88G [00:39<00:00, 77.9MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-05-31 03:13:24] Model ready (took 79.5 s)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚îÄ‚îÄ‚îÄ Set a preferred language if you want (or set to None for autodetect) ‚îÄ‚îÄ‚îÄ\n",
        "PREFERRED_LANGUAGE = \"zh\"  # or None to let Whisper auto-detect\n",
        "\n",
        "if PREFERRED_LANGUAGE:\n",
        "    print(f\"üåê Preferred language set to '{PREFERRED_LANGUAGE}'\")\n",
        "else:\n",
        "    print(\"üåê Using Whisper's automatic language detection\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Gather all audio files from INBOX_DIR ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "audio_files = [p for p in INBOX_DIR.iterdir() if p.suffix.lower() in AUDIO_EXT]\n",
        "total = len(audio_files)\n",
        "\n",
        "if total == 0:\n",
        "    print(\"üìÇ No audio in inbox. Drop files there and re-run this cell.\")\n",
        "else:\n",
        "    for idx, audio in enumerate(audio_files, 1):\n",
        "        print(f\"\\n[{_now()}] ‚ñ∂Ô∏è  ({idx}/{total})  {audio.name}\")\n",
        "\n",
        "        # Build Whisper kwargs\n",
        "        kwargs = dict(word_timestamps=True, verbose=True)\n",
        "        if PREFERRED_LANGUAGE:\n",
        "            kwargs[\"language\"] = PREFERRED_LANGUAGE\n",
        "            print(f\"[{_now()}] üîß Forcing language = '{PREFERRED_LANGUAGE}'\")\n",
        "        else:\n",
        "            print(f\"[{_now()}] üîç Auto language detection\")\n",
        "\n",
        "        # Transcribe ‚Äì this prints segments as it goes\n",
        "        result = model.transcribe(str(audio), **kwargs)\n",
        "\n",
        "        # Save transcript to TRANSCRIPTS_DIR\n",
        "        out_txt = TRANSCRIPTS_DIR / f\"{audio.stem}.txt\"\n",
        "        save_transcript(result, out_txt)\n",
        "        print(f\"[{_now()}] üìù Saved transcript ‚Üí {out_txt.name}\")\n",
        "\n",
        "        # Move original audio into PROCESSED_DIR\n",
        "        shutil.move(str(audio), PROCESSED_DIR / audio.name)\n",
        "        print(f\"[{_now()}] ‚úîÔ∏è Moved audio to processed/\")\n",
        "\n",
        "    print(\"\\nüéâ All transcription jobs finished!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmMP6U-pTLnE",
        "outputId": "e777f3e4-1f67-4436-cb36-b8b568bff707"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåê Preferred language set to 'zh'\n",
            "\n",
            "[2025-05-31 03:13:59] ‚ñ∂Ô∏è  (1/2)  ÈåÑË£Ω (5).m4a\n",
            "[2025-05-31 03:13:59] üîß Forcing language = 'zh'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:02.020] ÈåÑÈü≥Ê∏¨Ë©¶\n",
            "[00:02.020 --> 00:03.340] ÈåÑÈü≥Ê∏¨Ë©¶\n",
            "[00:04.000 --> 00:05.660] 123 123\n",
            "[00:06.360 --> 00:07.840] ÈåÑÈü≥Ê∏¨Ë©¶\n",
            "[2025-05-31 03:16:05] üìù Saved transcript ‚Üí ÈåÑË£Ω (5).txt\n",
            "[2025-05-31 03:16:05] ‚úîÔ∏è Moved audio to processed/\n",
            "\n",
            "[2025-05-31 03:16:05] ‚ñ∂Ô∏è  (2/2)  ÈåÑË£Ω (6).m4a\n",
            "[2025-05-31 03:16:05] üîß Forcing language = 'zh'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:07.000] Audio record test, audio record test, test, 1, 2, 3, test, 1, 2, 3\n",
            "[2025-05-31 03:17:57] üìù Saved transcript ‚Üí ÈåÑË£Ω (6).txt\n",
            "[2025-05-31 03:17:57] ‚úîÔ∏è Moved audio to processed/\n",
            "\n",
            "üéâ All transcription jobs finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datetime import timedelta\n",
        "\n",
        "# Regex to match lines like: [HH:MM:SS.mmm ‚Üí HH:MM:SS.mmm] text\n",
        "timestamp_pattern = re.compile(\n",
        "    r\"\\[(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\s*‚Üí\\s*(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\]\\s*(.*)\"\n",
        ")\n",
        "\n",
        "def parse_time(s: str) -> timedelta:\n",
        "    \"\"\"Convert 'HH:MM:SS.mmm' ‚Üí timedelta.\"\"\"\n",
        "    h, m, rest = s.split(\":\")\n",
        "    s_part, ms = rest.split(\".\")\n",
        "    return timedelta(hours=int(h), minutes=int(m),\n",
        "                     seconds=int(s_part), milliseconds=int(ms))\n",
        "\n",
        "def process_transcript(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Group segments every 5 minutes. Output format:\n",
        "      [HH:MM:SS.mmm]\n",
        "      <concatenated text for that 5-minute chunk>\n",
        "\n",
        "    Blank line between chunks.\n",
        "    \"\"\"\n",
        "    lines = text.splitlines()\n",
        "    segments = []\n",
        "    for line in lines:\n",
        "        m = timestamp_pattern.match(line)\n",
        "        if m:\n",
        "            start_ts, end_ts, content = m.groups()\n",
        "            segments.append((parse_time(start_ts), start_ts, content.strip()))\n",
        "\n",
        "    if not segments:\n",
        "        return \"\"  # no timed segments found\n",
        "\n",
        "    result = []\n",
        "    buffer = \"\"\n",
        "    last_mark_minute = None\n",
        "\n",
        "    for ts, start_ts_str, content in segments:\n",
        "        curr_minute = int(ts.total_seconds() // 300)  # chunk index (every 300 sec)\n",
        "        if last_mark_minute is None or curr_minute != last_mark_minute:\n",
        "            if buffer:\n",
        "                result.append(buffer.strip())\n",
        "                buffer = \"\"\n",
        "            result.append(f\"[{start_ts_str}]\")\n",
        "            last_mark_minute = curr_minute\n",
        "        buffer += content + \" \"\n",
        "\n",
        "    if buffer:\n",
        "        result.append(buffer.strip())\n",
        "\n",
        "    # Combine into blocks of 3 lines: timestamp, paragraph, blank line\n",
        "    output = []\n",
        "    for i in range(0, len(result), 2):\n",
        "        if i + 1 < len(result):\n",
        "            output.append(result[i])   # timestamp line\n",
        "            output.append(result[i+1]) # text for that block\n",
        "            output.append(\"\")          # blank line\n",
        "\n",
        "    return \"\\n\".join(output).strip()\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Batch processing: read every .txt in TRANSCRIPTS_DIR ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "for txtfile in TRANSCRIPTS_DIR.glob(\"*.txt\"):\n",
        "    with txtfile.open(encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    processed = process_transcript(text)\n",
        "    out_path = PARSED_DIR / txtfile.name.replace(\".txt\", \"_parsed.txt\")\n",
        "\n",
        "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(processed)\n",
        "    print(f\"üîß Processed {txtfile.name} ‚Üí {out_path.name}\")\n",
        "\n",
        "print(\"\\n‚úÖ All transcripts parsed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRTMjYI8QRE7",
        "outputId": "4e8d8d52-6daf-4698-9500-f338d475f866"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Processed ÈåÑË£Ω (5).txt ‚Üí ÈåÑË£Ω (5)_parsed.txt\n",
            "üîß Processed ÈåÑË£Ω (6).txt ‚Üí ÈåÑË£Ω (6)_parsed.txt\n",
            "\n",
            "‚úÖ All transcripts parsed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚îÄ‚îÄ‚îÄ System prompt remains unchanged ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "## System Prompt\n",
        "\n",
        "You are tasked with summarizing a speech provided in its original language. Create a concise, structured summary using clear and informative markdown formatting. Follow the outline and format precisely. You may use markdown tables, bullet points, paragraphs, or a combination as appropriate.\n",
        "\n",
        "## Summary Structure\n",
        "\n",
        "### Title\n",
        "\n",
        "Provide a concise, relevant title reflecting the key theme or message of the speech.\n",
        "Please make sure the title starts with a # to be recognized as title.\n",
        "\n",
        "# Title\n",
        "\n",
        "### Speaker\n",
        "\n",
        "* **Name**: \\\\[Speaker's Name]\n",
        "* **Affiliation/Role**: \\\\[Speaker‚Äôs Affiliation or Role, if known]\n",
        "* **Event**: \\\\[Event or occasion where the speech was given, if applicable]\n",
        "* **Date**: \\\\[Date of the speech, if available]\n",
        "\n",
        "### Overview\n",
        "\n",
        "Provide a short paragraph summarizing the overall purpose and main points of the speech.\n",
        "\n",
        "### Key Points\n",
        "\n",
        "Summarize each major point clearly. You may use markdown tables, bullet points, or paragraphs as needed:\n",
        "\n",
        "* **Key Point 1**: Brief description with supporting details.\n",
        "* **Key Point 2**: Brief description with supporting details.\n",
        "* Additional points as necessary.\n",
        "\n",
        "Or alternatively, use a markdown table.\n",
        "\n",
        "### Notable Quotes\n",
        "\n",
        "Include one or two significant quotes from the speech, if available, highlighting central themes or key statements made by the speaker:\n",
        "\n",
        "* *\"Quote 1...\"*\n",
        "* *\"Quote 2...\"*\n",
        "\n",
        "### Audience Reaction\n",
        "\n",
        "Briefly describe audience reactions, if mentioned or apparent (e.g., applause, questions raised, notable silence).\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Summarize briefly how the speaker concluded their speech and highlight any key takeaway messages.\n",
        "\n",
        "---\n",
        "\n",
        "Ensure clarity, accuracy, and conciseness in the summary, preserving essential context and meaning.\n",
        "Please summarize using the native language of the speech.\n",
        "\"\"\"\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Retrieve Gemini API key from Colab secrets ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "api_key = userdata.get('GEMINI_API_KEY')\n",
        "if api_key is None:\n",
        "    raise ValueError(\"GEMINI_API_KEY not found in Colab secrets! Add it via Settings ‚Üí Secrets.\")\n",
        "\n",
        "genai.configure(api_key=api_key)\n"
      ],
      "metadata": {
        "id": "yvalvnzdQT74"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def generate_summary_with_gemini(speech_text: str, system_prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Sends full_prompt = (system_prompt + speech_text) to Gemini-2.5-Flash.\n",
        "    Returns Gemini's textual response (Markdown).\n",
        "    \"\"\"\n",
        "    model = genai.GenerativeModel(\"gemini-2.5-flash-preview-05-20\")\n",
        "    full_prompt = system_prompt.strip() + \"\\n\\n\" + speech_text.strip()\n",
        "    try:\n",
        "        response = model.generate_content(\n",
        "            full_prompt,\n",
        "            generation_config=genai.types.GenerationConfig(temperature=0.5),\n",
        "            stream=False,\n",
        "        )\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Gemini API error: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_all_txt_files(parsed_dir: Path, markdown_dir: Path, system_prompt: str):\n",
        "    \"\"\"\n",
        "    For every .txt in parsed_dir, generate a summary via Gemini and save .md in markdown_dir.\n",
        "    \"\"\"\n",
        "    parsed_dir = Path(parsed_dir)\n",
        "    markdown_dir = Path(markdown_dir)\n",
        "    markdown_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    txt_files = list(parsed_dir.glob(\"*.txt\"))\n",
        "    print(f\"[DEBUG] Found {len(txt_files)} .txt files in {parsed_dir}\")\n",
        "\n",
        "    for txt_path in txt_files:\n",
        "        print(f\"\\n[DEBUG] Processing: {txt_path.name}\")\n",
        "        try:\n",
        "            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                speech_text = f.read().strip()\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Could not read {txt_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        if not speech_text:\n",
        "            print(f\"[WARNING] {txt_path.name} is empty, skipping.\")\n",
        "            continue\n",
        "\n",
        "        summary_md = generate_summary_with_gemini(speech_text, system_prompt)\n",
        "        if summary_md is None:\n",
        "            print(f\"[ERROR] Gemini API failed for {txt_path.name}, skipping.\")\n",
        "            continue\n",
        "\n",
        "        md_path = markdown_dir / (txt_path.stem + \".md\")\n",
        "        try:\n",
        "            with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(summary_md)\n",
        "            print(f\"[INFO] Saved summary ‚Üí {md_path.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Could not save {md_path}: {e}\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Run the batch summarization ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "process_all_txt_files(PARSED_DIR, MARKDOWN_DIR, SYSTEM_PROMPT)\n",
        "print(\"\\n‚úÖ All summaries generated.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "HZj8PaFVQZkR",
        "outputId": "10a72a72-14a1-4ded-8d30-704f6fbecfae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Found 2 .txt files in /content/drive/MyDrive/Whisper-STT-project/parsed\n",
            "\n",
            "[DEBUG] Processing: ÈåÑË£Ω (5)_parsed.txt\n",
            "[INFO] Saved summary ‚Üí ÈåÑË£Ω (5)_parsed.md\n",
            "\n",
            "[DEBUG] Processing: ÈåÑË£Ω (6)_parsed.txt\n",
            "[INFO] Saved summary ‚Üí ÈåÑË£Ω (6)_parsed.md\n",
            "\n",
            "‚úÖ All summaries generated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import shutil\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Retrieve HackMD token from Colab secrets ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "hackmd_token = userdata.get('HACKMD_TOKEN')\n",
        "if hackmd_token is None:\n",
        "    raise ValueError(\"HACKMD_TOKEN not found in Colab secrets! Add it via Settings ‚Üí Secrets.\")\n",
        "\n",
        "def upload_to_hackmd(md_content: str, filename: str, api_token: str) -> dict:\n",
        "    \"\"\"\n",
        "    Uploads a single markdown string to HackMD. Returns {\"title\":..., \"url\":...} on success.\n",
        "    \"\"\"\n",
        "    # Derive a clean title from the filename\n",
        "    if filename.endswith('.md'):\n",
        "        filename = filename[:-3]\n",
        "    raw_title = filename.replace('_parsed', '').strip()\n",
        "    title = raw_title.replace('_', ' ').strip()\n",
        "\n",
        "    # Ensure there's a top-level heading\n",
        "    md_lines = md_content.lstrip().splitlines()\n",
        "    if not md_lines or not md_lines[0].strip().startswith(\"# \"):\n",
        "        md_content = f\"# {title}\\n\\n\" + md_content.lstrip()\n",
        "    else:\n",
        "        md_lines[0] = f\"# {title}\"\n",
        "        md_content = \"\\n\".join(md_lines)\n",
        "\n",
        "    # Append hashtag if missing\n",
        "    hashtag = \"#whisper-stt-project\"\n",
        "    content_lines = md_content.rstrip().splitlines()\n",
        "    if not any(line.strip() == hashtag for line in content_lines[-3:]):\n",
        "        md_content = md_content.rstrip() + \"\\n\\n\" + hashtag + \"\\n\"\n",
        "\n",
        "    url = \"https://api.hackmd.io/v1/notes\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"title\": title,\n",
        "        \"content\": md_content,\n",
        "        \"readPermission\": \"guest\",\n",
        "        \"writePermission\": \"signed_in\"\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    if response.ok:\n",
        "        note_id = response.json().get(\"id\")\n",
        "        shared_url = f\"https://hackmd.io/{note_id}\"\n",
        "        print(f\"[INFO] Uploaded to HackMD: {shared_url}\")\n",
        "        return {\"title\": title, \"url\": shared_url}\n",
        "    else:\n",
        "        print(f\"[ERROR] HackMD upload failed for {filename}: {response.status_code} {response.text}\")\n",
        "        return None\n",
        "\n",
        "def batch_upload_markdown_and_move(markdown_dir: Path, uploaded_dir: Path, hackmd_token: str) -> list:\n",
        "    \"\"\"\n",
        "    For each .md in markdown_dir: upload via upload_to_hackmd ‚Üí move file to uploaded_dir.\n",
        "    Returns list of {\"title\":..., \"url\":...}.\n",
        "    \"\"\"\n",
        "    markdown_dir = Path(markdown_dir)\n",
        "    uploaded_dir = Path(uploaded_dir)\n",
        "    uploaded_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    md_files = list(markdown_dir.glob(\"*.md\"))\n",
        "    print(f\"[DEBUG] Found {len(md_files)} markdown files to upload.\")\n",
        "\n",
        "    shared_links = []\n",
        "    for md_file in md_files:\n",
        "        print(f\"[DEBUG] Processing: {md_file.name}\")\n",
        "        try:\n",
        "            with open(md_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                md_content = f.read()\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Could not read {md_file.name}: {e}\")\n",
        "            continue\n",
        "\n",
        "        result = upload_to_hackmd(md_content, md_file.name, hackmd_token)\n",
        "        if result:\n",
        "            shared_links.append(result)\n",
        "            dest_file = uploaded_dir / md_file.name\n",
        "            try:\n",
        "                shutil.move(str(md_file), dest_file)\n",
        "                print(f\"[INFO] Moved {md_file.name} ‚Üí {dest_file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] Failed to move {md_file.name}: {e}\")\n",
        "    return shared_links\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Run HackMD upload and collect shared links ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "shared_links = batch_upload_markdown_and_move(MARKDOWN_DIR, UPLOADED_DIR, hackmd_token)\n",
        "print(\"\\n‚úÖ All markdown files uploaded to HackMD.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbENeOkdQdvH",
        "outputId": "380ed571-c58b-43a4-a47c-13a368cbc939"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Found 2 markdown files to upload.\n",
            "[DEBUG] Processing: ÈåÑË£Ω (5)_parsed.md\n",
            "[INFO] Uploaded to HackMD: https://hackmd.io/S0MSTpwkSrWeZYie42EOWw\n",
            "[INFO] Moved ÈåÑË£Ω (5)_parsed.md ‚Üí /content/drive/MyDrive/Whisper-STT-project/uploaded/ÈåÑË£Ω (5)_parsed.md\n",
            "[DEBUG] Processing: ÈåÑË£Ω (6)_parsed.md\n",
            "[INFO] Uploaded to HackMD: https://hackmd.io/W_P6WdSRTsuJOU84wcOfVA\n",
            "[INFO] Moved ÈåÑË£Ω (6)_parsed.md ‚Üí /content/drive/MyDrive/Whisper-STT-project/uploaded/ÈåÑË£Ω (6)_parsed.md\n",
            "\n",
            "‚úÖ All markdown files uploaded to HackMD.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import smtplib\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "from email.header import Header\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ Retrieve email credentials from Colab secrets ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "email_user = userdata.get('EMAIL_USER')  # e.g. \"youremail@gmail.com\"\n",
        "email_pass = userdata.get('EMAIL_PASS')  # application-specific password or OAUTH token\n",
        "email_to   = userdata.get('EMAIL_TO')    # comma-separated list or single recipient\n",
        "\n",
        "if not (email_user and email_pass and email_to):\n",
        "    raise ValueError(\"EMAIL_USER, EMAIL_PASS, or EMAIL_TO not found in Colab secrets!\")\n",
        "\n",
        "subject = \"üìù Your Uploaded HackMD Speech Summaries\"\n",
        "\n",
        "# Build email body lines\n",
        "body_lines = [\n",
        "    \"Hello,\",\n",
        "    \"\",\n",
        "    \"Your audio files have been automatically transcribed using Whisper AI,\",\n",
        "    \"and the speech content was summarized using Gemini Flash 2.5.\",\n",
        "    \"\",\n",
        "    \"Here are the links to your uploaded speech summaries on HackMD:\",\n",
        "    \"\"\n",
        "]\n",
        "for link in shared_links:\n",
        "    body_lines.append(f\"- {link['title']}: {link['url']}\")\n",
        "body_lines += [\n",
        "    \"\",\n",
        "    \"All documents are shared and accessible to anyone with the link.\",\n",
        "    \"\",\n",
        "    \"If you have any questions or encounter any problems, feel free to reply to this email!\",\n",
        "    \"\",\n",
        "    \"Best regards,\",\n",
        "    \"Whisper-STT-Project Bot\"\n",
        "]\n",
        "body = \"\\n\".join(body_lines)\n",
        "\n",
        "# Compose the email\n",
        "msg = MIMEMultipart()\n",
        "msg['From'] = email_user\n",
        "msg['To']   = email_to\n",
        "msg['Subject'] = Header(subject, 'utf-8')\n",
        "msg.attach(MIMEText(body, 'plain', 'utf-8'))\n",
        "\n",
        "# Use Gmail's SMTP (SSL) on port 465\n",
        "with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:\n",
        "    server.login(email_user, email_pass)\n",
        "    server.send_message(msg)\n",
        "    print(\"[INFO] Email sent successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulhPQA0DQhZa",
        "outputId": "0c7550c3-87c1-4276-f685-7ebfef90cda4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Email sent successfully.\n"
          ]
        }
      ]
    }
  ]
}