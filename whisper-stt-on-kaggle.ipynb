{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/galenchen/whisper-stt-on-kaggle?scriptVersionId=243126861\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n!pip install --upgrade openai-whisper tqdm google-generativeai requests","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-06-01T19:43:43.476898Z","iopub.execute_input":"2025-06-01T19:43:43.477419Z","iopub.status.idle":"2025-06-01T19:45:24.438113Z","shell.execute_reply.started":"2025-06-01T19:43:43.477395Z","shell.execute_reply":"2025-06-01T19:45:24.437343Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.164.0)\nCollecting google-api-python-client\n  Downloading google_api_python_client-2.170.0-py3-none-any.whl.metadata (6.7 kB)\nRequirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (1.2.1)\nCollecting google-auth-oauthlib\n  Downloading google_auth_oauthlib-1.2.2-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.22.0)\nRequirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.40.1)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (1.34.1)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib) (2.0.0)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.20.3)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.0.9)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.4.26)\nDownloading google_api_python_client-2.170.0-py3-none-any.whl (13.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading google_auth_oauthlib-1.2.2-py3-none-any.whl (19 kB)\nInstalling collected packages: google-auth-oauthlib, google-api-python-client\n  Attempting uninstall: google-auth-oauthlib\n    Found existing installation: google-auth-oauthlib 1.2.1\n    Uninstalling google-auth-oauthlib-1.2.1:\n      Successfully uninstalled google-auth-oauthlib-1.2.1\n  Attempting uninstall: google-api-python-client\n    Found existing installation: google-api-python-client 2.164.0\n    Uninstalling google-api-python-client-2.164.0:\n      Successfully uninstalled google-api-python-client-2.164.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed google-api-python-client-2.170.0 google-auth-oauthlib-1.2.2\nCollecting openai-whisper\n  Downloading openai-whisper-20240930.tar.gz (800 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\nCollecting google-generativeai\n  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\nRequirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (1.26.4)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\nRequirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\nRequirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\nRequirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\nRequirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (1.34.1)\nRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.170.0)\nRequirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.40.1)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (3.20.3)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.4)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.2)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\nRequirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.72.0rc1)\nRequirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.49.0rc1)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.0.9)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->openai-whisper) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->openai-whisper) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->openai-whisper) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->openai-whisper) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->openai-whisper) (2024.2.0)\nDownloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hBuilding wheels for collected packages: openai-whisper\n  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803404 sha256=9aff015b0f7a8d4a8a18e75e548c8ea7cf6cd2ab1674c7db8db62f4c51099da2\n  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\nSuccessfully built openai-whisper\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, google-generativeai, openai-whisper\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n  Attempting uninstall: google-generativeai\n    Found existing installation: google-generativeai 0.8.4\n    Uninstalling google-generativeai-0.8.4:\n      Successfully uninstalled google-generativeai-0.8.4\nSuccessfully installed google-generativeai-0.8.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# 3 important folders' id for file input and output\n\nfrom kaggle_secrets import UserSecretsClient\ns = UserSecretsClient()\n\nto_be_transcribed = s.get_secret(\"TO_BE_TRANSCRIBED\")\ntranscribed = s.get_secret(\"TRANSCRIBED\")\nPROCESSED_ID = s.get_secret(\"PROCESSED\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T19:47:22.953137Z","iopub.execute_input":"2025-06-01T19:47:22.953836Z","iopub.status.idle":"2025-06-01T19:47:23.728111Z","shell.execute_reply.started":"2025-06-01T19:47:22.95381Z","shell.execute_reply":"2025-06-01T19:47:23.727358Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import json\nfrom google.oauth2 import service_account\nfrom googleapiclient.discovery import build\n\n# 1. Pull the SERVICE_ACCOUNT JSON from Kaggle Secrets and write it to /kaggle/working\n# ──────────────────────────────────────────────────────────────────────────────\nsa_json_str = s.get_secret(\"GDRIVE_SA_JSON\")\n\nsa_path = \"/kaggle/working/sa_key.json\"\nwith open(sa_path, \"w\") as f:\n    f.write(sa_json_str)\n\n# ──────────────────────────────────────────────────────────────────────────────\n# 2. Build the Drive client using that file\n# ──────────────────────────────────────────────────────────────────────────────\nSCOPES = [\"https://www.googleapis.com/auth/drive\"]\ncreds = service_account.Credentials.from_service_account_file(sa_path, scopes=SCOPES)\ndrive_service = build(\"drive\", \"v3\", credentials=creds)\n\n# ──────────────────────────────────────────────────────────────────────────────\n# 3. Remove the local service account file for security\n# ──────────────────────────────────────────────────────────────────────────────\nif os.path.exists(sa_path):\n    try:\n        os.remove(sa_path)\n        print(f\"✅ Removal successful: '{sa_path}' has been deleted.\")\n    except Exception as e:\n        print(f\"❌ Removal failed: {e}\")\nelse:\n    print(f\"ℹ️ No such file: '{sa_path}' (nothing to remove).\")\n\n# ──────────────────────────────────────────────────────────────────────────────\n# 4. List files in the shared folder, then set new_files flag accordingly\n# ──────────────────────────────────────────────────────────────────────────────\nFOLDER_ID = to_be_transcribed  # ← replace with your actual folder ID\n\ndef list_files_in_folder(folder_id):\n    query = f\"'{folder_id}' in parents and trashed=false\"\n    files = []\n    page_token = None\n    while True:\n        resp = drive_service.files().list(\n            q=query,\n            spaces=\"drive\",\n            fields=\"nextPageToken, files(id, name)\",\n            pageToken=page_token\n        ).execute()\n        files.extend(resp.get(\"files\", []))\n        page_token = resp.get(\"nextPageToken\", None)\n        if not page_token:\n            break\n    return files\n\nfiles = list_files_in_folder(FOLDER_ID)\n\n# Determine whether new files exist\nif not files:\n    new_files = False\n    print(f\"⚠️ No files found in folder ID = {FOLDER_ID!r}. new_files = False.\")\nelse:\n    new_files = True\n    print(f\"✅ Found {len(files)} file(s) in folder ID = {FOLDER_ID!r}. new_files = True.\")\n    for f in files:\n        print(f\" • {f['name']} (ID={f['id']})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T19:47:27.879745Z","iopub.execute_input":"2025-06-01T19:47:27.879998Z","iopub.status.idle":"2025-06-01T19:47:30.256549Z","shell.execute_reply.started":"2025-06-01T19:47:27.879982Z","shell.execute_reply":"2025-06-01T19:47:30.255809Z"}},"outputs":[{"name":"stdout","text":"✅ Removal successful: '/kaggle/working/sa_key.json' has been deleted.\n✅ Found 1 file(s) in folder ID = '1AKnppHssmAwkjBo2EPI8Twf6782hH2xv'. new_files = True.\n • 20220115我跟恩美的故事❤️.m4a (ID=1JoD_VUgOOF2-P_4O2mN06qP4FiOccIVH)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport io\nfrom googleapiclient.http import MediaIoBaseDownload\n\n# ──────────────────────────────────────────────────────────────────────────────\n# 1. Replace this with your actual Drive folder ID (the folder containing all files to download)\nTO_BE_TRANSCRIBED_ID = to_be_transcribed\n\n# 2. Create a local folder under /kaggle/working\nlocal_root = \"/kaggle/working/from_google_drive\"\nos.makedirs(local_root, exist_ok=True)\n\n# ──────────────────────────────────────────────────────────────────────────────\n# 3. List all non-folder files directly under TO_BE_TRANSCRIBED_ID\nquery_files = (\n    f\"'{TO_BE_TRANSCRIBED_ID}' in parents and \"\n    \"trashed = false and \"\n    \"mimeType != 'application/vnd.google-apps.folder'\"\n)\nresponse = drive_service.files().list(\n    q=query_files,\n    spaces=\"drive\",\n    fields=\"files(id, name)\"\n).execute()\nfiles = response.get(\"files\", [])\n\nif not files:\n    print(f\"⚠️ No files found in folder ID = {TO_BE_TRANSCRIBED_ID!r}, skip downloading.\")\nelse:\n    print(f\"🔎 Found {len(files)} file(s) in folder ID = {TO_BE_TRANSCRIBED_ID!r}:\")\n    for f in files:\n        print(f\" • {f['name']}  (ID = {f['id']})\")\n\n# ──────────────────────────────────────────────────────────────────────────────\n# 4. Download each file into /kaggle/working/from_google_drive\nfor f in files:\n    file_id   = f[\"id\"]\n    file_name = f[\"name\"]\n    dest_path = os.path.join(local_root, file_name)\n\n    try:\n        request = drive_service.files().get_media(fileId=file_id)\n        fh = io.FileIO(dest_path, mode=\"wb\")\n        downloader = MediaIoBaseDownload(fh, request)\n        done = False\n        print(f\"⬇️  Downloading {file_name!r} …\")\n        while not done:\n            status, done = downloader.next_chunk()\n        fh.close()\n        print(f\"✅ Saved to {dest_path}\")\n    except Exception as e:\n        print(f\"❌ Error downloading {file_name!r}: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T19:49:30.968526Z","iopub.execute_input":"2025-06-01T19:49:30.968872Z","iopub.status.idle":"2025-06-01T19:49:38.944613Z","shell.execute_reply.started":"2025-06-01T19:49:30.968849Z","shell.execute_reply":"2025-06-01T19:49:38.943962Z"}},"outputs":[{"name":"stdout","text":"🔎 Found 1 file(s) in folder ID = '1AKnppHssmAwkjBo2EPI8Twf6782hH2xv':\n • 20220115我跟恩美的故事❤️.m4a  (ID = 1JoD_VUgOOF2-P_4O2mN06qP4FiOccIVH)\n⬇️  Downloading '20220115我跟恩美的故事❤️.m4a' …\n✅ Saved to /kaggle/working/from_google_drive/20220115我跟恩美的故事❤️.m4a\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import datetime, time\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport whisper\n\n# ========== Utility Functions ==========\n\ndef _now():\n    \"\"\"Log-friendly timestamp.\"\"\"\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\ndef _fmt_ts(seconds: float) -> str:\n    \"\"\"float seconds → HH:MM:SS.mmm string.\"\"\"\n    h, m = divmod(int(seconds), 3600)\n    m, s = divmod(m, 60)\n    ms = int((seconds - int(seconds)) * 1000)\n    return f\"{h:02d}:{m:02d}:{s:02d}.{ms:03d}\"\n\ndef save_transcript(result: dict, out_path: Path):\n    \"\"\"\n    Write Whisper's segment list to a .txt file.\n    Each line: [start → end] text\n    \"\"\"\n    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n        for seg in result[\"segments\"]:\n            f.write(f\"[{_fmt_ts(seg['start'])} → {_fmt_ts(seg['end'])}] {seg['text'].strip()}\\n\")\n\n# ========== Config ==========\n\nAUDIO_EXT = {'.wav', '.mp3', '.m4a', '.flac', '.ogg', '.webm'}\nINBOX_DIR = Path(\"/kaggle/working/from_google_drive\")\nTRANSCRIPTS_DIR = Path(\"/kaggle/working/transcription\")\nCACHE_DIR = Path(\"/kaggle/working/whisper_models\")\nPREFERRED_LANGUAGE = \"zh\"  # Change to None for autodetect\n\n# Ensure directories exist\nTRANSCRIPTS_DIR.mkdir(parents=True, exist_ok=True)\nCACHE_DIR.mkdir(parents=True, exist_ok=True)\n\n# ========== Gather Audio Files ==========\n\naudio_files = [p for p in INBOX_DIR.rglob(\"*\") if p.suffix.lower() in AUDIO_EXT]\ntotal = len(audio_files)\n\nif total == 0:\n    print(f\"[{_now()}] 📂 No audio files found under {INBOX_DIR}\")\n    print(\"Please place your audio files in /kaggle/working/from_google_drive and re-run.\")\n    new_files = False\nelse:\n    new_files = True\n\n# ========== Main Transcription Block ==========\n\nif not new_files:\n    print(f\"[{_now()}] ⚠️ new_files == False → Skipping transcription.\")\nelse:\n    # --- Load Whisper Model, using CACHE_DIR for both load & download ---\n    start = time.time()\n    print(f\"[{_now()}] Attempting to load Whisper large-v3 from cache…\")\n    try:\n        model = whisper.load_model(\"large-v3\", download_root=str(CACHE_DIR))\n        print(f\"[{_now()}] Model loaded from cache or downloaded into '{CACHE_DIR}'.\")\n    except Exception as e:\n        print(f\"[{_now()}] ⚠️ Failed to load from cache: {e}\")\n        print(f\"[{_now()}] Downloading Whisper large-v3 into '{CACHE_DIR}'…\")\n        model = whisper.load_model(\"large-v3\", download_root=str(CACHE_DIR))\n        print(f\"[{_now()}] Model downloaded and cached at '{CACHE_DIR}'.\")\n\n    print(f\"[{_now()}] Model ready (took {time.time() - start:.1f} s)\\n\")\n\n    # --- Set preferred language (or None for autodetect) ---\n    if PREFERRED_LANGUAGE:\n        print(f\"[{_now()}] 🌐 Preferred language set to '{PREFERRED_LANGUAGE}'\")\n    else:\n        print(f\"[{_now()}] 🌐 Using Whisper's automatic language detection\")\n\n    print(f\"[{_now()}] 🎧 Found {total} audio file(s) under {INBOX_DIR}\")\n\n    # --- Transcription loop with active tqdm progress bar per file ---\n    with tqdm(audio_files, desc=\"Transcribing files\", unit=\"file\") as pbar:\n        for audio in pbar:\n            pbar.set_description(f\"Transcribing: {audio.name}\")\n            # Build Whisper kwargs\n            kwargs = {\"word_timestamps\": True, \"verbose\": False}\n            if PREFERRED_LANGUAGE:\n                kwargs[\"language\"] = PREFERRED_LANGUAGE\n\n            # Transcribe\n            result = model.transcribe(str(audio), **kwargs)\n\n            # Save transcript with same stem under TRANSCRIPTS_DIR\n            out_txt = TRANSCRIPTS_DIR / f\"{audio.stem}.txt\"\n            save_transcript(result, out_txt)\n\n    print(f\"\\n[{_now()}] 🎉 All transcription jobs finished! Transcripts are in {TRANSCRIPTS_DIR}\")\n","metadata":{"trusted":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2025-06-01T19:54:19.51715Z","iopub.execute_input":"2025-06-01T19:54:19.517849Z","iopub.status.idle":"2025-06-01T20:13:45.25315Z","shell.execute_reply.started":"2025-06-01T19:54:19.517824Z","shell.execute_reply":"2025-06-01T20:13:45.252424Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"[2025-06-01 19:54:19] Attempting to load Whisper large-v3 from cache…\n[2025-06-01 19:54:43] Model loaded from cache or downloaded into '/kaggle/working/whisper_models'.\n[2025-06-01 19:54:43] Model ready (took 24.1 s)\n\n[2025-06-01 19:54:43] 🌐 Preferred language set to 'zh'\n[2025-06-01 19:54:43] 🎧 Found 1 audio file(s) under /kaggle/working/from_google_drive\n","output_type":"stream"},{"name":"stderr","text":"Transcribing: 20220115我跟恩美的故事❤️.m4a:   0%|          | 0/1 [00:00<?, ?file/s]\n  0%|          | 0/367084 [00:00<?, ?frames/s]\u001b[A\n  1%|          | 2830/367084 [00:11<23:44, 255.79frames/s]\u001b[A\n  2%|▏         | 5830/367084 [00:16<16:12, 371.38frames/s]\u001b[A\n  2%|▏         | 8830/367084 [00:23<14:40, 406.69frames/s]\u001b[A\n  3%|▎         | 11830/367084 [00:30<14:01, 421.92frames/s]\u001b[A\n  4%|▍         | 14830/367084 [00:36<13:27, 436.35frames/s]\u001b[A\n  5%|▍         | 17830/367084 [00:43<13:30, 430.68frames/s]\u001b[A\n  6%|▌         | 20830/367084 [00:49<12:44, 453.16frames/s]\u001b[A\n  6%|▋         | 23830/367084 [00:54<11:54, 480.08frames/s]\u001b[A\n  7%|▋         | 26830/367084 [01:00<11:16, 502.87frames/s]\u001b[A\n  8%|▊         | 29830/367084 [01:07<11:57, 469.96frames/s]\u001b[A\n  9%|▉         | 32592/367084 [01:14<12:42, 438.65frames/s]\u001b[A\n 10%|▉         | 35592/367084 [01:22<13:00, 424.87frames/s]\u001b[A\n 11%|█         | 38592/367084 [01:29<12:38, 433.08frames/s]\u001b[A\n 11%|█         | 38592/367084 [01:39<12:38, 433.08frames/s]\u001b[A\n 11%|█▏        | 41592/367084 [01:54<22:35, 240.14frames/s]\u001b[A\n 12%|█▏        | 44592/367084 [02:00<18:55, 284.05frames/s]\u001b[A\n 13%|█▎        | 47592/367084 [02:06<16:34, 321.37frames/s]\u001b[A\n 14%|█▍        | 50592/367084 [02:12<14:39, 359.96frames/s]\u001b[A\n 14%|█▍        | 53194/367084 [02:19<14:11, 368.47frames/s]\u001b[A\n 15%|█▌        | 56194/367084 [02:26<13:11, 392.96frames/s]\u001b[A\n 16%|█▌        | 59194/367084 [02:38<15:45, 325.74frames/s]\u001b[A\n 17%|█▋        | 62192/367084 [02:45<14:20, 354.26frames/s]\u001b[A\n 18%|█▊        | 64962/367084 [02:51<13:17, 378.90frames/s]\u001b[A\n 19%|█▊        | 67962/367084 [02:57<12:16, 405.87frames/s]\u001b[A\n 19%|█▉        | 70962/367084 [03:04<11:49, 417.18frames/s]\u001b[A\n 20%|██        | 73962/367084 [03:11<11:31, 423.96frames/s]\u001b[A\n 21%|██        | 76650/367084 [03:18<11:40, 414.67frames/s]\u001b[A\n 22%|██▏       | 79650/367084 [03:25<11:22, 421.05frames/s]\u001b[A\n 23%|██▎       | 82650/367084 [03:31<11:03, 428.93frames/s]\u001b[A\n 23%|██▎       | 85302/367084 [03:39<11:45, 399.63frames/s]\u001b[A\n 24%|██▍       | 88302/367084 [03:46<11:30, 403.84frames/s]\u001b[A\n 25%|██▍       | 91302/367084 [03:53<11:02, 416.21frames/s]\u001b[A\n 26%|██▌       | 94302/367084 [04:01<11:01, 412.14frames/s]\u001b[A\n 26%|██▌       | 94302/367084 [04:19<11:01, 412.14frames/s]\u001b[A\n 26%|██▋       | 96990/367084 [04:24<18:46, 239.70frames/s]\u001b[A\n 26%|██▋       | 96990/367084 [04:39<18:46, 239.70frames/s]\u001b[A\n 27%|██▋       | 99990/367084 [04:50<24:59, 178.16frames/s]\u001b[A\n 28%|██▊       | 102842/367084 [04:56<20:02, 219.69frames/s]\u001b[A\n 29%|██▉       | 105842/367084 [05:03<16:59, 256.33frames/s]\u001b[A\n 30%|██▉       | 108760/367084 [05:12<15:21, 280.33frames/s]\u001b[A\n 30%|███       | 111760/367084 [05:19<13:35, 312.94frames/s]\u001b[A\n 31%|███       | 114664/367084 [05:25<12:00, 350.54frames/s]\u001b[A\n 32%|███▏      | 117664/367084 [05:30<10:42, 388.36frames/s]\u001b[A\n 33%|███▎      | 120664/367084 [05:37<10:10, 403.33frames/s]\u001b[A\n 34%|███▎      | 123452/367084 [05:43<09:31, 425.95frames/s]\u001b[A\n 34%|███▍      | 126452/367084 [05:49<09:07, 439.33frames/s]\u001b[A\n 35%|███▌      | 129452/367084 [05:55<08:40, 456.55frames/s]\u001b[A\n 36%|███▌      | 132452/367084 [06:02<08:37, 453.33frames/s]\u001b[A\n 37%|███▋      | 135452/367084 [06:07<07:52, 489.96frames/s]\u001b[A\n 38%|███▊      | 138452/367084 [06:14<08:23, 454.13frames/s]\u001b[A\n 38%|███▊      | 138452/367084 [06:29<08:23, 454.13frames/s]\u001b[A\n 39%|███▊      | 141452/367084 [06:38<14:39, 256.63frames/s]\u001b[A\n 39%|███▉      | 144452/367084 [06:44<12:19, 301.08frames/s]\u001b[A\n 39%|███▉      | 144452/367084 [06:59<12:19, 301.08frames/s]\u001b[A\n 40%|████      | 147450/367084 [07:34<26:47, 136.60frames/s]\u001b[A\n 41%|████      | 150446/367084 [07:41<20:56, 172.40frames/s]\u001b[A\n 41%|████      | 150446/367084 [07:59<20:56, 172.40frames/s]\u001b[A\n 42%|████▏     | 153176/367084 [08:29<32:38, 109.24frames/s]\u001b[A\n 43%|████▎     | 156114/367084 [08:35<24:42, 142.35frames/s]\u001b[A\n 43%|████▎     | 159074/367084 [08:42<19:18, 179.49frames/s]\u001b[A\n 44%|████▍     | 162074/367084 [08:49<15:36, 218.87frames/s]\u001b[A\n 45%|████▍     | 165074/367084 [08:54<12:34, 267.81frames/s]\u001b[A\n 46%|████▌     | 168074/367084 [09:02<11:09, 297.08frames/s]\u001b[A\n 47%|████▋     | 171074/367084 [09:09<10:03, 324.91frames/s]\u001b[A\n 47%|████▋     | 173984/367084 [09:16<09:14, 348.15frames/s]\u001b[A\n 48%|████▊     | 176984/367084 [09:23<08:43, 362.81frames/s]\u001b[A\n 48%|████▊     | 176984/367084 [09:39<08:43, 362.81frames/s]\u001b[A\n 49%|████▉     | 179462/367084 [10:09<21:34, 144.91frames/s]\u001b[A\n 50%|████▉     | 182338/367084 [10:16<16:57, 181.59frames/s]\u001b[A\n 50%|█████     | 185308/367084 [10:23<13:52, 218.28frames/s]\u001b[A\n 51%|█████▏    | 188308/367084 [10:31<11:58, 248.83frames/s]\u001b[A\n 51%|█████▏    | 188308/367084 [10:49<11:58, 248.83frames/s]\u001b[A\n 52%|█████▏    | 190590/367084 [11:15<22:50, 128.79frames/s]\u001b[A\n 53%|█████▎    | 193344/367084 [11:21<17:34, 164.77frames/s]\u001b[A\n 53%|█████▎    | 196344/367084 [11:36<16:22, 173.85frames/s]\u001b[A\n 54%|█████▍    | 199344/367084 [11:41<12:24, 225.35frames/s]\u001b[A\n 55%|█████▌    | 202344/367084 [11:47<10:08, 270.63frames/s]\u001b[A\n 56%|█████▌    | 205344/367084 [11:51<08:14, 327.12frames/s]\u001b[A\n 56%|█████▌    | 205344/367084 [12:09<08:14, 327.12frames/s]\u001b[A\n 57%|█████▋    | 208312/367084 [12:13<11:24, 231.79frames/s]\u001b[A\n 58%|█████▊    | 211200/367084 [12:19<09:30, 273.32frames/s]\u001b[A\n 58%|█████▊    | 214200/367084 [12:25<08:04, 315.30frames/s]\u001b[A\n 59%|█████▉    | 217200/367084 [12:29<06:27, 386.65frames/s]\u001b[A\n 60%|█████▉    | 220200/367084 [12:34<05:38, 434.42frames/s]\u001b[A\n 61%|██████    | 223200/367084 [12:40<05:20, 448.82frames/s]\u001b[A\n 62%|██████▏   | 226200/367084 [12:45<04:48, 488.21frames/s]\u001b[A\n 62%|██████▏   | 229200/367084 [12:51<04:45, 482.46frames/s]\u001b[A\n 63%|██████▎   | 232200/367084 [12:56<04:25, 508.22frames/s]\u001b[A\n 64%|██████▍   | 235200/367084 [13:03<04:31, 485.53frames/s]\u001b[A\n 65%|██████▍   | 238200/367084 [13:08<04:10, 513.86frames/s]\u001b[A\n 66%|██████▌   | 241200/367084 [13:15<04:14, 494.53frames/s]\u001b[A\n 67%|██████▋   | 244200/367084 [13:21<04:07, 496.36frames/s]\u001b[A\n 67%|██████▋   | 247200/367084 [13:26<03:51, 518.92frames/s]\u001b[A\n 68%|██████▊   | 250200/367084 [13:34<04:06, 474.41frames/s]\u001b[A\n 69%|██████▉   | 253200/367084 [13:41<04:07, 459.51frames/s]\u001b[A\n 70%|██████▉   | 255958/367084 [13:46<03:55, 472.34frames/s]\u001b[A\n 71%|███████   | 258958/367084 [13:53<03:56, 456.30frames/s]\u001b[A\n 71%|███████▏  | 261958/367084 [14:01<03:59, 439.52frames/s]\u001b[A\n 72%|███████▏  | 264594/367084 [14:08<04:03, 420.77frames/s]\u001b[A\n 73%|███████▎  | 267594/367084 [14:14<03:54, 424.29frames/s]\u001b[A\n 74%|███████▎  | 270594/367084 [14:20<03:31, 457.09frames/s]\u001b[A\n 74%|███████▍  | 273350/367084 [14:26<03:28, 449.42frames/s]\u001b[A\n 75%|███████▌  | 276350/367084 [14:33<03:21, 450.08frames/s]\u001b[A\n 76%|███████▌  | 279350/367084 [14:40<03:17, 444.65frames/s]\u001b[A\n 77%|███████▋  | 281952/367084 [14:47<03:18, 428.09frames/s]\u001b[A\n 78%|███████▊  | 284952/367084 [14:52<02:55, 468.14frames/s]\u001b[A\n 78%|███████▊  | 287952/367084 [14:58<02:47, 473.32frames/s]\u001b[A\n 79%|███████▉  | 290952/367084 [15:04<02:40, 473.44frames/s]\u001b[A\n 80%|████████  | 293952/367084 [15:09<02:27, 495.85frames/s]\u001b[A\n 81%|████████  | 296952/367084 [15:15<02:15, 517.63frames/s]\u001b[A\n 82%|████████▏ | 299952/367084 [15:20<02:06, 528.95frames/s]\u001b[A\n 83%|████████▎ | 302952/367084 [15:24<01:52, 572.30frames/s]\u001b[A\n 83%|████████▎ | 305952/367084 [15:29<01:45, 578.80frames/s]\u001b[A\n 84%|████████▍ | 308666/367084 [15:34<01:42, 572.20frames/s]\u001b[A\n 85%|████████▍ | 311666/367084 [15:39<01:33, 590.73frames/s]\u001b[A\n 86%|████████▌ | 314666/367084 [15:45<01:32, 568.02frames/s]\u001b[A\n 87%|████████▋ | 317666/367084 [15:51<01:33, 529.87frames/s]\u001b[A\n 87%|████████▋ | 320408/367084 [15:56<01:25, 543.27frames/s]\u001b[A\n 88%|████████▊ | 323408/367084 [16:03<01:27, 500.52frames/s]\u001b[A\n 89%|████████▉ | 326408/367084 [16:08<01:18, 515.00frames/s]\u001b[A\n 90%|████████▉ | 329408/367084 [16:15<01:13, 509.70frames/s]\u001b[A\n 91%|█████████ | 332408/367084 [16:21<01:10, 488.41frames/s]\u001b[A\n 91%|█████████▏| 335408/367084 [16:27<01:03, 502.60frames/s]\u001b[A\n 92%|█████████▏| 338408/367084 [16:34<01:01, 469.69frames/s]\u001b[A\n 93%|█████████▎| 341408/367084 [16:41<00:55, 460.94frames/s]\u001b[A\n 94%|█████████▍| 344408/367084 [16:55<01:06, 338.77frames/s]\u001b[A\n 94%|█████████▍| 344408/367084 [17:09<01:06, 338.77frames/s]\u001b[A\n 94%|█████████▍| 346874/367084 [17:13<01:21, 247.98frames/s]\u001b[A\n 95%|█████████▌| 349750/367084 [17:20<01:02, 279.06frames/s]\u001b[A\n 96%|█████████▌| 352748/367084 [17:28<00:47, 303.03frames/s]\u001b[A\n 96%|█████████▌| 352748/367084 [17:39<00:47, 303.03frames/s]\u001b[A\n 97%|█████████▋| 355748/367084 [17:43<00:43, 261.59frames/s]\u001b[A\n 98%|█████████▊| 358748/367084 [17:51<00:28, 291.60frames/s]\u001b[A\n 98%|█████████▊| 358748/367084 [18:09<00:28, 291.60frames/s]\u001b[A\n 99%|█████████▊| 361748/367084 [18:40<00:39, 136.20frames/s]\u001b[A\n 99%|█████████▉| 364676/367084 [18:48<00:14, 167.69frames/s]\u001b[A\n100%|██████████| 367084/367084 [18:54<00:00, 323.61frames/s]\u001b[A\nTranscribing: 20220115我跟恩美的故事❤️.m4a: 100%|██████████| 1/1 [19:01<00:00, 1141.59s/file]","output_type":"stream"},{"name":"stdout","text":"\n[2025-06-01 20:13:45] 🎉 All transcription jobs finished! Transcripts are in /kaggle/working/transcription\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import re\nfrom datetime import timedelta, datetime\nfrom pathlib import Path\n\n# Regex to match lines like: [HH:MM:SS.mmm → HH:MM:SS.mmm] text\ntimestamp_pattern = re.compile(\n    r\"\\[(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\s*→\\s*(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\]\\s*(.*)\"\n)\n\ndef parse_time(s: str) -> timedelta:\n    \"\"\"Convert 'HH:MM:SS.mmm' → timedelta.\"\"\"\n    h, m, rest = s.split(\":\")\n    s_part, ms = rest.split(\".\")\n    return timedelta(hours=int(h), minutes=int(m),\n                     seconds=int(s_part), milliseconds=int(ms))\n\ndef process_transcript(text: str) -> str:\n    \"\"\"\n    Group segments every 5 minutes. Output format:\n      [HH:MM:SS.mmm]\n      <concatenated text for that 5-minute chunk>\n\n    Blank line between chunks.\n    \"\"\"\n    lines = text.splitlines()\n    segments = []\n    for line in lines:\n        m = timestamp_pattern.match(line)\n        if m:\n            start_ts, end_ts, content = m.groups()\n            segments.append((parse_time(start_ts), start_ts, content.strip()))\n\n    if not segments:\n        return \"\"  # no timed segments found\n\n    result = []\n    buffer = \"\"\n    last_mark_minute = None\n\n    for ts, start_ts_str, content in segments:\n        curr_minute = int(ts.total_seconds() // 300)  # chunk index (every 300 sec)\n        if last_mark_minute is None or curr_minute != last_mark_minute:\n            if buffer:\n                result.append(buffer.strip())\n                buffer = \"\"\n            result.append(f\"[{start_ts_str}]\")\n            last_mark_minute = curr_minute\n        buffer += content + \" \"\n\n    if buffer:\n        result.append(buffer.strip())\n\n    # Combine into blocks of 3 lines: timestamp, paragraph, blank line\n    output = []\n    for i in range(0, len(result), 2):\n        if i + 1 < len(result):\n            output.append(result[i])   # timestamp line\n            output.append(result[i+1]) # text for that block\n            output.append(\"\")          # blank line\n\n    return \"\\n\".join(output).strip()\n\n# ─── Batch processing for Kaggle ─────────────\nTRANSCRIPTS_DIR = Path(\"/kaggle/working/transcription\")\nPARSED_DIR = Path(\"/kaggle/working/parsed\")\nPARSED_DIR.mkdir(parents=True, exist_ok=True)\n\ntxt_files = list(TRANSCRIPTS_DIR.glob(\"*.txt\"))\nif not txt_files:\n    print(f\"[{datetime.now():%Y-%m-%d %H:%M:%S}] ⚠️ No .txt files found in {TRANSCRIPTS_DIR}. Skipping parsing.\")\nelse:\n    for txtfile in txt_files:\n        with txtfile.open(encoding=\"utf-8\") as f:\n            text = f.read()\n\n        processed = process_transcript(text)\n        out_path = PARSED_DIR / txtfile.name.replace(\".txt\", \"_parsed.txt\")\n\n        with out_path.open(\"w\", encoding=\"utf-8\") as f:\n            f.write(processed)\n        print(f\"🔧 Processed {txtfile.name} → {out_path.name}\")\n\n    print(\"\\n✅ All transcripts parsed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T20:15:30.232009Z","iopub.execute_input":"2025-06-01T20:15:30.232839Z","iopub.status.idle":"2025-06-01T20:15:30.253184Z","shell.execute_reply.started":"2025-06-01T20:15:30.232813Z","shell.execute_reply":"2025-06-01T20:15:30.252355Z"}},"outputs":[{"name":"stdout","text":"🔧 Processed 20220115我跟恩美的故事❤️.txt → 20220115我跟恩美的故事❤️_parsed.txt\n\n✅ All transcripts parsed.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from googleapiclient.discovery import build\n\nDOC_ID = '1p44XUpBu7lPjyux4eANd_9FHT5F1UDbgUyx7q6Libvk'\n\ndef get_doc_text(doc_id: str, creds) -> str:\n    service = build('docs', 'v1', credentials=creds)\n    doc = service.documents().get(documentId=doc_id).execute()\n    text = []\n    for element in doc.get('body', {}).get('content', []):\n        if 'paragraph' in element:\n            for run in element['paragraph'].get('elements', []):\n                txt = run.get('textRun', {}).get('content')\n                if txt:\n                    text.append(txt)\n    return ''.join(text).strip()\n\n# Retrieve the system prompt from the Google Doc\nSYSTEM_PROMPT = get_doc_text(DOC_ID, creds)\nprint(\"[INFO] SYSTEM_PROMPT loaded from Google Doc. Preview:\\n\", SYSTEM_PROMPT)\n","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-06-01T20:15:37.171796Z","iopub.execute_input":"2025-06-01T20:15:37.172521Z","iopub.status.idle":"2025-06-01T20:15:38.777495Z","shell.execute_reply.started":"2025-06-01T20:15:37.172485Z","shell.execute_reply":"2025-06-01T20:15:38.776877Z"}},"outputs":[{"name":"stdout","text":"[INFO] SYSTEM_PROMPT loaded from Google Doc. Preview:\n ## System Prompt\n\nYou are tasked with summarizing a speech provided in its original language. Create a concise, structured summary using clear and informative markdown formatting. Follow the outline and format precisely. You may use markdown tables, bullet points, paragraphs, or a combination as appropriate.\n\n## Summary Structure\n\n### Title\n\nProvide a concise, relevant title reflecting the key theme or message of the speech.\nPlease make sure the title starts with a # to be recognized as title.\n\n# Title\n\n### Speaker\n\n* **Name**: \\\\[Speaker's Name]\n* **Affiliation/Role**: \\\\[Speaker’s Affiliation or Role, if known]\n* **Event**: \\\\[Event or occasion where the speech was given, if applicable]\n* **Date**: \\\\[Date of the speech, if available]\n\n### Overview\n\nProvide a short paragraph summarizing the overall purpose and main points of the speech.\n\n### Key Points\n\nSummarize each major point clearly. You may use markdown tables, bullet points, or paragraphs as needed:\n\n* **Key Point 1**: Brief description with supporting details.\n* **Key Point 2**: Brief description with supporting details.\n* Additional points as necessary.\n\nOr alternatively, use a markdown table.\n\n### Notable Quotes\n\nInclude one or two significant quotes from the speech, if available, highlighting central themes or key statements made by the speaker:\n\n* *\"Quote 1...\"*\n* *\"Quote 2...\"*\n\n### Audience Reaction\n\nBriefly describe audience reactions, if mentioned or apparent (e.g., applause, questions raised, notable silence).\n\n### Conclusion\n\nSummarize briefly how the speaker concluded their speech and highlight any key takeaway messages.\n\n---\n\nEnsure clarity, accuracy, and conciseness in the summary, preserving essential context and meaning.\nPlease summarize using the native language of the speech.\n\"\"\"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from pathlib import Path\nfrom kaggle_secrets import UserSecretsClient\nimport google.generativeai as genai\n\n# Retrieve Gemini API key from Kaggle secrets\ns = UserSecretsClient()\napi_key = s.get_secret(\"GEMINI_API_KEY\")\nif api_key is None:\n    raise ValueError(\"GEMINI_API_KEY not found in Kaggle secrets! Add it via Add-ons → Secrets.\")\n\ngenai.configure(api_key=api_key)\n\ndef generate_summary_with_gemini(speech_text: str, system_prompt: str) -> str:\n    model = genai.GenerativeModel(\"gemini-2.5-flash-preview-05-20\")\n    full_prompt = system_prompt.strip() + \"\\n\\n\" + speech_text.strip()\n    try:\n        response = model.generate_content(\n            full_prompt,\n            generation_config=genai.types.GenerationConfig(temperature=0.5),\n            stream=False,\n        )\n        return response.text\n    except Exception as e:\n        print(f\"[ERROR] Gemini API error: {e}\")\n        return None\n\ndef process_all_txt_files(parsed_dir: Path, markdown_dir: Path, system_prompt: str):\n    parsed_dir = Path(parsed_dir)\n    markdown_dir = Path(markdown_dir)\n    markdown_dir.mkdir(parents=True, exist_ok=True)\n\n    txt_files = list(parsed_dir.glob(\"*.txt\"))\n    if not txt_files:\n        print(f\"[INFO] No parsed .txt files found in {parsed_dir}. Skipping summarization.\")\n        return\n\n    print(f\"[INFO] Found {len(txt_files)} .txt files in {parsed_dir}\")\n\n    for txt_path in txt_files:\n        print(f\"\\n[INFO] Processing: {txt_path.name}\")\n        try:\n            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n                speech_text = f.read().strip()\n        except Exception as e:\n            print(f\"[ERROR] Could not read {txt_path}: {e}\")\n            continue\n\n        if not speech_text:\n            print(f\"[WARNING] {txt_path.name} is empty, skipping.\")\n            continue\n\n        summary_md = generate_summary_with_gemini(speech_text, system_prompt)\n        if summary_md is None:\n            print(f\"[ERROR] Gemini API failed for {txt_path.name}, skipping.\")\n            continue\n\n        md_path = markdown_dir / (txt_path.stem.replace(\"_parsed\", \"\") + \".md\")\n\n        try:\n            with open(md_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(summary_md)\n            print(f\"[INFO] Saved summary → {md_path.name}\")\n        except Exception as e:\n            print(f\"[ERROR] Could not save {md_path}: {e}\")\n\n# Set your input/output directories and system prompt\nPARSED_DIR = Path(\"/kaggle/working/parsed\")\nMARKDOWN_DIR = Path(\"/kaggle/working/markdown\")\n# SYSTEM_PROMPT = ... # (get from your Google Doc as before)\n\n# Only run if there are files to process\nif list(PARSED_DIR.glob(\"*.txt\")):\n    process_all_txt_files(PARSED_DIR, MARKDOWN_DIR, SYSTEM_PROMPT)\n    print(\"\\n✅ All summaries generated.\")\nelse:\n    print(f\"[INFO] No .txt files found in {PARSED_DIR}, nothing to summarize.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T20:15:45.047756Z","iopub.execute_input":"2025-06-01T20:15:45.04802Z","iopub.status.idle":"2025-06-01T20:16:07.85447Z","shell.execute_reply.started":"2025-06-01T20:15:45.048003Z","shell.execute_reply":"2025-06-01T20:16:07.85383Z"}},"outputs":[{"name":"stdout","text":"[INFO] Found 1 .txt files in /kaggle/working/parsed\n\n[INFO] Processing: 20220115我跟恩美的故事❤️_parsed.txt\n[INFO] Saved summary → 20220115我跟恩美的故事❤️.md\n\n✅ All summaries generated.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import requests\nimport shutil\nfrom pathlib import Path\nfrom kaggle_secrets import UserSecretsClient\n\n# ─── Retrieve HackMD token from Kaggle secrets ───────────────────────────\ns = UserSecretsClient()\nhackmd_token = s.get_secret(\"HACKMD_TOKEN\")\nif hackmd_token is None:\n    raise ValueError(\"HACKMD_TOKEN not found in Kaggle secrets! Add it via Add-ons → Secrets.\")\n\ndef upload_to_hackmd(md_content: str, filename: str, api_token: str) -> dict:\n    \"\"\"\n    Uploads a single markdown string to HackMD. Returns {\"title\":..., \"url\":...} on success.\n    \"\"\"\n    # Derive a clean title from the filename\n    if filename.endswith('.md'):\n        filename = filename[:-3]\n    raw_title = filename.replace('_parsed', '').strip()\n    title = raw_title.replace('_', ' ').strip()\n\n    # Ensure there's a top-level heading\n    md_lines = md_content.lstrip().splitlines()\n    if not md_lines or not md_lines[0].strip().startswith(\"# \"):\n        md_content = f\"# {title}\\n\\n\" + md_content.lstrip()\n    else:\n        md_lines[0] = f\"# {title}\"\n        md_content = \"\\n\".join(md_lines)\n\n    # Append hashtag if missing\n    hashtag = \"#whisper-stt-project\"\n    content_lines = md_content.rstrip().splitlines()\n    if not any(line.strip() == hashtag for line in content_lines[-3:]):\n        md_content = md_content.rstrip() + \"\\n\\n\" + hashtag + \"\\n\"\n\n    url = \"https://api.hackmd.io/v1/notes\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_token}\",\n        \"Content-Type\": \"application/json\"\n    }\n    data = {\n        \"title\": title,\n        \"content\": md_content,\n        \"readPermission\": \"guest\",\n        \"writePermission\": \"signed_in\"\n    }\n    response = requests.post(url, headers=headers, json=data)\n    if response.ok:\n        note_id = response.json().get(\"id\")\n        shared_url = f\"https://hackmd.io/{note_id}\"\n        print(f\"[INFO] Uploaded to HackMD: {shared_url}\")\n        return {\"title\": title, \"url\": shared_url}\n    else:\n        print(f\"[ERROR] HackMD upload failed for {filename}: {response.status_code} {response.text}\")\n        return None\n\ndef batch_upload_markdown_and_move(markdown_dir: Path, uploaded_dir: Path, hackmd_token: str) -> list:\n    \"\"\"\n    For each .md in markdown_dir: upload via upload_to_hackmd → move file to uploaded_dir.\n    Returns list of {\"title\":..., \"url\":...}.\n    \"\"\"\n    markdown_dir = Path(markdown_dir)\n    uploaded_dir = Path(uploaded_dir)\n    uploaded_dir.mkdir(parents=True, exist_ok=True)\n\n    md_files = list(markdown_dir.glob(\"*.md\"))\n    if not md_files:\n        print(f\"[INFO] No markdown files found in {markdown_dir}, skipping upload.\")\n        return []\n\n    print(f\"[INFO] Found {len(md_files)} markdown files to upload.\")\n\n    shared_links = []\n    for md_file in md_files:\n        print(f\"[INFO] Processing: {md_file.name}\")\n        try:\n            with open(md_file, \"r\", encoding=\"utf-8\") as f:\n                md_content = f.read()\n        except Exception as e:\n            print(f\"[ERROR] Could not read {md_file.name}: {e}\")\n            continue\n\n        result = upload_to_hackmd(md_content, md_file.name, hackmd_token)\n        if result:\n            shared_links.append(result)\n            dest_file = uploaded_dir / md_file.name\n            try:\n                shutil.move(str(md_file), dest_file)\n                print(f\"[INFO] Moved {md_file.name} → {dest_file}\")\n            except Exception as e:\n                print(f\"[ERROR] Failed to move {md_file.name}: {e}\")\n    return shared_links\n\n# ─── Set directories and run HackMD upload if there are files ──────────\nMARKDOWN_DIR = Path(\"/kaggle/working/markdown\")\nUPLOADED_DIR = Path(\"/kaggle/working/uploaded\")\n\nif list(MARKDOWN_DIR.glob(\"*.md\")):\n    shared_links = batch_upload_markdown_and_move(MARKDOWN_DIR, UPLOADED_DIR, hackmd_token)\n    print(\"\\n✅ All markdown files uploaded to HackMD.\")\nelse:\n    print(f\"[INFO] No .md files found in {MARKDOWN_DIR}, nothing to upload.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T20:20:07.779412Z","iopub.execute_input":"2025-06-01T20:20:07.779741Z","iopub.status.idle":"2025-06-01T20:20:09.021194Z","shell.execute_reply.started":"2025-06-01T20:20:07.779685Z","shell.execute_reply":"2025-06-01T20:20:09.020367Z"}},"outputs":[{"name":"stdout","text":"[INFO] Found 1 markdown files to upload.\n[INFO] Processing: 20220115我跟恩美的故事❤️.md\n[INFO] Uploaded to HackMD: https://hackmd.io/y0vWysRIS1KjRmHIq2REnA\n[INFO] Moved 20220115我跟恩美的故事❤️.md → /kaggle/working/uploaded/20220115我跟恩美的故事❤️.md\n\n✅ All markdown files uploaded to HackMD.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ╔══════════════════════════════════════════════════════════════════╗\n# 0) CONSTANT GOOGLE-DRIVE IDS (do NOT change names)                 #\n# ╚══════════════════════════════════════════════════════════════════╝\nINBOX_ID        = s.get_secret(\"TO_BE_TRANSCRIBED\") # to_be_transcribed\nARCHIVE_ID      = s.get_secret(\"TRANSCRIBED\")  # transcribed\nPROCESSED_ID    = s.get_secret(\"PROCESSED\")  # processed data (text/md)\n\n# ╔══════════════════════════════════════════════════════════════════╗\n# 1) LOCAL WORKING PATHS                                             #\n# ╚══════════════════════════════════════════════════════════════════╝\nfrom pathlib import Path, PurePath\nimport shutil, datetime\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload\nfrom googleapiclient.errors import HttpError\n\nWORKING      = Path(\"/kaggle/working\")\nTRANS_DIR    = WORKING / \"transcription\"\nPARSED_DIR   = WORKING / \"parsed\"\nUPLOADED_MD  = WORKING / \"uploaded\"            # markdown from HackMD step\nAUDIO_LOCAL  = WORKING / \"from_google_drive\"   # downloaded audio\n\ndrive = build(\"drive\", \"v3\", credentials=creds)\n\ndef log(msg): print(f\"[{datetime.datetime.now():%H:%M:%S}] {msg}\")\n\n# ╔══════════════════════════════════════════════════════════════════╗\n# 2) HELPERS                                                         #\n# ╚══════════════════════════════════════════════════════════════════╝\ndef ensure_subfolder(parent_id: str, name: str) -> str:\n    \"\"\"Return id of subfolder 'name' under parent, creating if absent.\"\"\"\n    q = (f\"'{parent_id}' in parents and mimeType='application/vnd.google-apps.folder' \"\n         f\"and name='{name}' and trashed=false\")\n    res = drive.files().list(q=q, spaces=\"drive\", fields=\"files(id)\").execute()\n    if res[\"files\"]:\n        return res[\"files\"][0][\"id\"]\n    meta = {\"name\": name,\n            \"mimeType\": \"application/vnd.google-apps.folder\",\n            \"parents\": [parent_id]}\n    return drive.files().create(body=meta, fields=\"id\").execute()[\"id\"]\n\ndef upload_file(local: Path, parent_id: str):\n    media = MediaFileUpload(local, resumable=False)\n    meta  = {\"name\": local.name, \"parents\": [parent_id]}\n    drive.files().create(body=meta, media_body=media, fields=\"id\").execute()\n    log(f\"  ↳ uploaded {local.name}\")\n\ndef move_audio(audio_name: str):\n# Move one audio file (exact name) from inbox → archive; silent if not found.\n    q = (f\"'{INBOX_ID}' in parents and name='{audio_name}' and trashed=false\")\n    res = drive.files().list(\n        q=q, spaces=\"drive\", fields=\"files(id)\"\n    ).execute().get(\"files\", [])\n    \n    if not res:\n        return                              # silent if not found\n    fid = res[0][\"id\"]\n    drive.files().update(\n        fileId=fid,\n        addParents=ARCHIVE_ID,\n        removeParents=INBOX_ID,\n        fields=\"id\"\n    ).execute()\n    log(f\"  ↳ moved {audio_name} → transcribed\")\n\n# ╔══════════════════════════════════════════════════════════════════╗\n# 3) PROCESS MARKDOWN FILES                                          #\n# ╚══════════════════════════════════════════════════════════════════╝\nmd_files = list(UPLOADED_MD.glob(\"*.md\"))\nif not md_files:\n    log(\"ℹ️  No markdown files in /uploaded – nothing to sync.\")\nelse:\n    for md in md_files:\n        stem = md.stem\n        folder_id = ensure_subfolder(PROCESSED_ID, stem)\n        log(f\"📂 Drive subfolder '{stem}' (id {folder_id})\")\n\n        txt_path    = TRANS_DIR  / f\"{stem}.txt\"\n        parsed_path = PARSED_DIR / f\"{stem}_parsed.txt\"\n\n        for p in (txt_path, parsed_path, md):\n            if p.exists():\n                upload_file(p, folder_id)\n\n        # Move corresponding audio (same base name, keep original extension)\n        for audio_local in AUDIO_LOCAL.glob(f\"{stem}.*\"):\n            if audio_local.is_file():\n                move_audio(audio_local.name)\n                break   # move only first match\n\n        # Verify contents\n        present = {f[\"name\"] for f in drive.files().list(\n            q=f\"'{folder_id}' in parents and trashed=false\",\n            spaces=\"drive\", fields=\"files(name)\").execute()[\"files\"]}\n        expected = {txt_path.name, parsed_path.name, md.name}\n        if expected - present:\n            log(f\"  ✖ missing {expected - present}\")\n        else:\n            log(\"  ✅ files verified\")\n\n        md.unlink(missing_ok=True)   # remove local markdown\n\n# ╔══════════════════════════════════════════════════════════════════╗\n# 4) MOVE EXTRA AUDIO BASED ON LOCAL COPIES (if any)                 #\n# ╚══════════════════════════════════════════════════════════════════╝\nfor audio_local in AUDIO_LOCAL.glob(\"*\"):\n    if audio_local.is_file():\n        move_audio(audio_local.name)\n\n# ╔══════════════════════════════════════════════════════════════════╗\n# 5) CLEAN KAGGLE WORKSPACE (keep whisper_models)                    #\n# ╚══════════════════════════════════════════════════════════════════╝\nlog(\"🧹 Cleaning /kaggle/working (keeping whisper_models)\")\nfor item in WORKING.iterdir():\n    if item.name == \"whisper_models\":\n        continue\n    try:\n        shutil.rmtree(item) if item.is_dir() else item.unlink()\n    except Exception as e:\n        log(f\"  ✖ could not delete {item}: {e}\")\nlog(\"✅ All done\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T20:20:18.359347Z","iopub.execute_input":"2025-06-01T20:20:18.360077Z","iopub.status.idle":"2025-06-01T20:20:29.349762Z","shell.execute_reply.started":"2025-06-01T20:20:18.36005Z","shell.execute_reply":"2025-06-01T20:20:29.349099Z"}},"outputs":[{"name":"stdout","text":"[20:20:21] 📂 Drive subfolder '20220115我跟恩美的故事❤️' (id 13B5Y7mD9s2fCmuYRy7_Z-iJFHNeiKR0K)\n[20:20:23]   ↳ uploaded 20220115我跟恩美的故事❤️.txt\n[20:20:24]   ↳ uploaded 20220115我跟恩美的故事❤️_parsed.txt\n[20:20:26]   ↳ uploaded 20220115我跟恩美的故事❤️.md\n[20:20:27]   ↳ moved 20220115我跟恩美的故事❤️.m4a → transcribed\n[20:20:27]   ✅ files verified\n[20:20:28] 🧹 Cleaning /kaggle/working (keeping whisper_models)\n[20:20:29] ✅ All done\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ╔══════════════════════════════════════════════════════════════════╗\n#  EMAIL HACKMD LINKS  (only if something was uploaded)              #\n# ╚══════════════════════════════════════════════════════════════════╝\nfrom kaggle_secrets import UserSecretsClient\nimport smtplib\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nfrom email.header import Header\n\n# --- Skip entirely if no links were produced -----------------------\nif not (globals().get(\"shared_links\") and shared_links):\n    print(\"[INFO] No uploaded Markdown links – skipping email step.\")\nelse:\n    # --- Retrieve secrets ------------------------------------------\n    s = UserSecretsClient()\n    email_user = s.get_secret(\"EMAIL_USER\")\n    email_pass = s.get_secret(\"EMAIL_PASS\")\n    email_to   = s.get_secret(\"EMAIL_TO\")\n\n    if not all([email_user, email_pass, email_to]):\n        print(\"[WARN] Email secrets missing – email not sent.\")\n    else:\n        # --- Build email body --------------------------------------\n        subject = \"📝 Your Uploaded HackMD Speech Summaries\"\n        body_lines = [\n            \"Hello,\",\n            \"\",\n            \"Your audio files were transcribed with Whisper and\",\n            \"summarized using Gemini Flash 2.5. The summaries are now\",\n            \"available on HackMD:\",\n            \"\"\n        ] + [f\"- {link['title']}: {link['url']}\" for link in shared_links] + [\n            \"\",\n            \"If you have questions just reply to this email.\",\n            \"\",\n            \"Best regards,\",\n            \"Whisper-STT Bot\"\n        ]\n        body = \"\\n\".join(body_lines)\n\n        # --- Compose & send ---------------------------------------\n        msg             = MIMEMultipart()\n        msg[\"From\"]     = email_user\n        msg[\"To\"]       = email_to\n        msg[\"Subject\"]  = Header(subject, \"utf-8\")\n        msg.attach(MIMEText(body, \"plain\", \"utf-8\"))\n\n        try:\n            with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465) as server:\n                server.login(email_user, email_pass)\n                server.send_message(msg)\n            print(\"[INFO] Email sent successfully.\")\n        except Exception as e:\n            print(f\"[ERROR] Email send failed: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T20:20:33.05128Z","iopub.execute_input":"2025-06-01T20:20:33.052035Z","iopub.status.idle":"2025-06-01T20:20:37.407514Z","shell.execute_reply.started":"2025-06-01T20:20:33.052012Z","shell.execute_reply":"2025-06-01T20:20:37.406795Z"}},"outputs":[{"name":"stdout","text":"[INFO] Email sent successfully.\n","output_type":"stream"}],"execution_count":15}]}